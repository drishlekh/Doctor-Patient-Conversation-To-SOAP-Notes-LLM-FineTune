{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niQper0y-Kfp"
      },
      "outputs": [],
      "source": [
        "pip install unsloth transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt"
      ],
      "metadata": {
        "id": "qZC7nc5g-PgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer=FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True\n",
        ")"
      ],
      "metadata": {
        "id": "us1IuyTN-Pic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=FastLanguageModel.get_peft_model(\n",
        "    model, r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "AmRFC_Xz-Pk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer= get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.2\")\n"
      ],
      "metadata": {
        "id": "qY2JVxct-Pns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(\"omi-health/medical-dialogue-to-soap-summary\", split=\"train\")"
      ],
      "metadata": {
        "id": "lQZYk3yR-wJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=standardize_sharegpt(dataset)"
      ],
      "metadata": {
        "id": "mcGZZhSj-wMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "enitcYil-wO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(dataset[0])\n",
        "dataset[0]"
      ],
      "metadata": {
        "id": "hFMKNJli-wRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert dialogue + soap into OpenAI-style conversation\n",
        "# def convert_to_conversation(example):\n",
        "#     return {\n",
        "#         \"conversations\": [\n",
        "#             {\"role\": \"user\", \"content\": example[\"dialogue\"]},\n",
        "#             {\"role\": \"assistant\", \"content\": example[\"soap\"]}\n",
        "#         ]\n",
        "#     }\n",
        "\n",
        "# dataset = dataset.map(convert_to_conversation)\n",
        "\n",
        "# # Now apply chat template\n",
        "# dataset = dataset.map(\n",
        "#     lambda examples: {\n",
        "#         \"text\": [\n",
        "#             tokenizer.apply_chat_template(examples[\"conversations\"], tokenize=False)\n",
        "#         ]\n",
        "#     }\n",
        "# )\n",
        "# Convert dialogue + soap into chat-style\n",
        "def convert_to_conversation(example):\n",
        "    return {\n",
        "        \"text\": tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": example[\"dialogue\"]},\n",
        "                {\"role\": \"assistant\", \"content\": example[\"soap\"]}\n",
        "            ],\n",
        "            tokenize=False\n",
        "        )\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(convert_to_conversation)\n"
      ],
      "metadata": {
        "id": "LKPn5vZk-wTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "c-qWlvs3-5PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fix the text column to be a single string, not a list\n",
        "# dataset = dataset.map(\n",
        "#     lambda example: {\n",
        "#         \"text\": tokenizer.apply_chat_template(example[\"conversations\"], tokenize=False)\n",
        "#     }\n",
        "# )\n",
        "# Convert dialogue + soap into chat-style format\n",
        "def convert_to_conversation(example):\n",
        "    return {\n",
        "        \"text\": tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": example[\"dialogue\"]},\n",
        "                {\"role\": \"assistant\", \"content\": example[\"soap\"]}\n",
        "            ],\n",
        "            tokenize=False\n",
        "        )\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(convert_to_conversation)\n"
      ],
      "metadata": {
        "id": "qDMnsX7d-5Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0][\"text\"])\n"
      ],
      "metadata": {
        "id": "rUZ54pUO-5T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "LzGte1vk-5Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./outputs\",\n",
        "#     per_device_train_batch_size=2,\n",
        "#     gradient_accumulation_steps=1,\n",
        "#     num_train_epochs=1,        # quick test\n",
        "#     max_steps=50,              # stop after 50 steps\n",
        "#     logging_steps=5,           # show loss every 5 steps\n",
        "#     save_steps=50,\n",
        "#     learning_rate=2e-5,\n",
        "#     fp16=True,\n",
        "#     bf16=False,\n",
        "#     report_to=\"none\"           # ✅ no wandb\n",
        "# )\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./outputs\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,   # effectively batch=8\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "0LZgDy1pAagY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset = dataset.select(range(500))  # use only first 500 examples\n"
      ],
      "metadata": {
        "id": "QhJ_1QIvFUXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=small_dataset,   # ✅ smaller dataset\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,            # shorter input → faster\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "aGbY6Ipr-5ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model + tokenizer\n",
        "save_dir = \"./finetuned_model\"\n",
        "trainer.save_model(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n"
      ],
      "metadata": {
        "id": "I-106HfABAQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for new:\n",
        "#\n",
        "#\n",
        "#\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load base model in 4bit again\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Load your LoRA adapters\n",
        "from peft import PeftModel\n",
        "model = PeftModel.from_pretrained(model, \"./finetuned_model\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./finetuned_model\")\n"
      ],
      "metadata": {
        "id": "4HhxcjoPL6l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# =========================\n",
        "# 1. Load finetuned model\n",
        "# =========================\n",
        "inference_model, inference_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"./finetuned_model\",   # path to your saved finetuned model\n",
        "    max_seq_length=2048,              # same as training\n",
        "    load_in_4bit=True                 # efficient inference\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "afZFRt52D0s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 2. Example conversation\n",
        "# =========================\n",
        "dialogue = \"\"\"Doctor: Hello, how can I help you today?\n",
        "Patient: My son has been having some issues with speech and development. He's 13 years old now.\n",
        "Doctor: I see. Can you tell me more about his symptoms? Does he have any issues with muscle tone or hypotonia?\n",
        "Patient: No, he doesn't have hypotonia. But he has mild to moderate speech and developmental delay, and he's been diagnosed with attention deficit disorder.\n",
        "Doctor: Thank you for sharing that information. We'll run some tests, including an MRI, to get a better understanding of your son's condition.\n",
        "(After the tests)\n",
        "Doctor: The MRI results are in, and I'm glad to say that there are no structural brain anomalies. However, I did notice some physical characteristics. Does your son have any facial features like retrognathia, mild hypertelorism, or a slightly elongated philtrum and thin upper lip?\n",
        "Patient: Yes, he has all of those features. His hands are also broad and short. And his feet have mild syndactyly of the second and third toe, with a sandal gap in both feet.\n",
        "Doctor: Thank you for confirming that. We also conducted Whole Exome Sequencing (WES) analyses, and we found a de novo frameshift variant in his genetic makeup. Specifically, it's Chr1(GRCh37):g.244217335del, NM_205768.2(ZBTB18):c.259del(p.(Leu87Cysfs*21)). This leads to a premature termination codon located more than 400 codons upstream of the canonical termination codon.\n",
        "Patient: What does that mean for my son?\n",
        "Doctor: This genetic variant may be contributing to your son's speech, developmental delay, and attention deficit disorder. It's important that we continue monitoring his progress and provide appropriate support for his development.\n",
        "Patient: What should we do for follow-up?\n",
        "Doctor: Regular visits with a speech and language therapist, an occupational therapist, and a psychologist can help address your son's developmental and attention deficit disorder needs. I will also recommend regular check-ups with me to monitor his growth and overall health.\n",
        "Patient: Thank you, doctor. We will follow your recommendations and keep an eye on his progress.\"\"\"\n",
        "\n",
        "# Wrap the dialogue as a user prompt\n",
        "formatted_prompt = inference_tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": dialogue + \"\\n\\nConvert the above doctor-patient conversation into a structured SOAP note\"}\n",
        "    ],\n",
        "    tokenize=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 3. Tokenize input\n",
        "# =========================\n",
        "model_inputs = inference_tokenizer(\n",
        "    formatted_prompt,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# =========================\n",
        "# 4. Generate response\n",
        "# =========================\n",
        "generated_ids = inference_model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,   # creativity\n",
        "    do_sample=True,\n",
        "    pad_token_id=inference_tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. Decode response\n",
        "# =========================\n",
        "response = inference_tokenizer.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True\n",
        ")[0]\n",
        "\n",
        "print(\"=== Model Response ===\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "IMZT-RA6Hefz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 2. Example conversation\n",
        "# =========================\n",
        "dialogue = \"\"\"Doctor: Hello, Patient D. How are you feeling today?\n",
        "Patient D: I'm feeling fine, doc, no complaints. Just here for a regular check-up.\n",
        "Doctor: That's good to hear. I see that you are a 60-year-old African American male and currently asymptomatic. I also noticed that your 62-year-old brother had prostate cancer and was successfully treated with radiation. Is that correct?\n",
        "Patient D: Yes, that's correct. My brother went through radiation treatment and is doing well now.\n",
        "Doctor: I'm glad to hear that your brother is doing well. Given your African American background and having a first-degree relative diagnosed with prostate cancer before 65 years of age, it's important to discuss the possibility of PSA testing with you.\n",
        "Patient D: Hmm, okay. What exactly is PSA testing, doc?\n",
        "Doctor: PSA testing, or Prostate-Specific Antigen testing, is a blood test that helps detect the presence of prostate cancer. However, the recommendations for PSA-based screening vary among different organizations.\n",
        "Patient D: So, do I need to get this screening done?\n",
        "Doctor: According to the AAFP, NCI, CDC, ACS, AUA, NCCN, and ASCO, it would be less difficult for clinicians to discuss PSA testing with you, considering your background and family history. These organizations recommend that we have a conversation about the potential benefits and risks of the test.\n",
        "Patient D: Alright, I see. What do the USPSTF recommendations say?\n",
        "Doctor: The USPSTF recommendations may not recommend PSA-based screening for you. It is important to understand that PSA testing has both potential benefits and risks, and the decision to undergo the test should be made after discussing these factors with your healthcare provider.\n",
        "Patient D: I understand. So, what should I do next, doc?\n",
        "Doctor: I recommend that we have a more detailed conversation about the potential benefits and risks of PSA testing, so you can make an informed decision. You may want to take some time to think about it and do some research before we discuss further.\n",
        "Patient D: Okay, that sounds like a good plan. I'll do some more research and get back to you with any questions or concerns.\n",
        "Doctor: That's a great approach. In the meantime, if you have any symptoms or concerns, please don't hesitate to reach out to us. I'll be here to help and guide you through the process.\n",
        "Patient D: Thank you, doc. I appreciate your help and guidance.\n",
        "Doctor: You're welcome, Patient D. I look forward to our next discussion. Take care and feel free to reach out if you have any questions.\n",
        "Patient D: Will do. Thanks again, and see you soon.\"\"\"\n",
        "\n",
        "# Wrap the dialogue as a user prompt\n",
        "formatted_prompt = inference_tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": dialogue + \"\\n\\nConvert the above doctor-patient conversation into a structured SOAP note\"}\n",
        "    ],\n",
        "    tokenize=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 3. Tokenize input\n",
        "# =========================\n",
        "model_inputs = inference_tokenizer(\n",
        "    formatted_prompt,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# =========================\n",
        "# 4. Generate response\n",
        "# =========================\n",
        "generated_ids = inference_model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,   # creativity\n",
        "    do_sample=True,\n",
        "    pad_token_id=inference_tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. Decode response\n",
        "# =========================\n",
        "response = inference_tokenizer.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True\n",
        ")[0]\n",
        "\n",
        "print(\"=== Model Response ===\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "L7b_31BCBATg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# =========================\n",
        "# 1. Load base (not fine-tuned) model\n",
        "# =========================\n",
        "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",  # original base model\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 2. Load fine-tuned model\n",
        "# =========================\n",
        "ft_model, ft_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"./finetuned_model\",  # your saved model dir\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 3. Example doctor-patient dialogue\n",
        "# =========================\n",
        "dialogue = \"\"\"Doctor: Good morning. What brings you in today?\n",
        "Patient: I’ve had a sore throat and mild fever for the last three days.\n",
        "Doctor: Do you have any cough or difficulty swallowing?\n",
        "Patient: Yes, I have a dry cough and it hurts a little when I swallow.\n",
        "Doctor: Any history of allergies or recent sick contacts?\n",
        "Patient: No allergies, but my son had a cold last week.\n",
        "Doctor: I see. Let’s take a look at your throat.\n",
        "\"\"\"\n",
        "\n",
        "# Wrap prompt\n",
        "formatted_prompt = ft_tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": dialogue + \"\\n\\nConvert the above doctor-patient conversation into a structured SOAP note\"}\n",
        "    ],\n",
        "    tokenize=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4. Helper for generation\n",
        "# =========================\n",
        "def generate(model, tokenizer, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# =========================\n",
        "# 5. Run both models\n",
        "# =========================\n",
        "base_output = generate(base_model, base_tokenizer, formatted_prompt)\n",
        "ft_output   = generate(ft_model, ft_tokenizer, formatted_prompt)\n",
        "\n",
        "print(\"=== Base Model (Before Fine-tuning) ===\")\n",
        "print(base_output)\n",
        "print(\"\\n\\n=== Fine-tuned Model (After Fine-tuning) ===\")\n",
        "print(ft_output)\n"
      ],
      "metadata": {
        "id": "xS8rE4bWWwT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/finetuned_model\"\n",
        "trainer.save_model(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n"
      ],
      "metadata": {
        "id": "sCgCkUQZXop1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/finetuned_model /content/drive/MyDrive/finetuned_model\n"
      ],
      "metadata": {
        "id": "b0rSX2XhXqmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Path where you saved in Drive\n",
        "model_path = \"/content/drive/MyDrive/finetuned_model\"\n",
        "\n",
        "# Reload fine-tuned model directly\n",
        "inference_model, inference_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "_HcIPSecXzc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nbstripout"
      ],
      "metadata": {
        "id": "YO8bpdhsjpeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nbstripout LLM_FineTune.ipynb"
      ],
      "metadata": {
        "id": "yABhuPvRjrnS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}